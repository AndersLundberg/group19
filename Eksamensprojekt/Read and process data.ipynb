{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import requests, json\n",
    "import pandas as pd, numpy as np\n",
    "import time, os\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepros(html) :\n",
    "    d=json.loads(html)\n",
    "    \n",
    "    soup = BeautifulSoup(d['result_list_box_html'],'lxml')\n",
    "    \n",
    "    # this selects the main part of the html\n",
    "    tabel = soup.find('div', attrs={'class':'results component--default'})\n",
    "    \n",
    "    # Jobindex contain both own postings and postings from other\n",
    "    # the two are treated differently in the data\n",
    "    # Jobindex' imported postings\n",
    "    others = re.compile('r[\\d]+')\n",
    "    tabel1 = tabel.findAll('div', attrs={'data-beacon-tid':others})\n",
    "    #print(tabel1)\n",
    "\n",
    "    # Jobindex' own postings\n",
    "    own = re.compile('h[\\d]+')\n",
    "    tabel2 = tabel.findAll('div', attrs={'data-beacon-tid':own})\n",
    "    #print(tabel2)\n",
    "    \n",
    "    return tabel, tabel1, tabel2\n",
    "\n",
    "def job_descr_own(tabel2) : # den er ikke færdig: tag <ul> og saml elementer, der hører sammen\n",
    "    desc_own = []\n",
    "\n",
    "    # tag ul skal med \n",
    "    for l in tabel2 :\n",
    "        g = l.findAll('p')\n",
    "        for m in g :\n",
    "            desc_own.append(m.text)\n",
    "    return desc_own\n",
    "\n",
    "def job_descr_oth(tabel1) : # færdig og virker\n",
    "    desc_oth = []\n",
    "    p=re.compile('\"')\n",
    "    lineshift = re.compile('\\n')\n",
    "    besk = []\n",
    "    for t_ in tabel1 :\n",
    "        besk.append(lineshift.sub(\"\", t_.text))\n",
    "    for t in besk :\n",
    "        desc_oth.append(t.split(sep='    ')[1])\n",
    "    return desc_oth\n",
    "    \n",
    "def job_title_oth (tabel) : # færdig og virker\n",
    "    # udled jobs andre\n",
    "    jobs_oth = []\n",
    "    j = tabel.findAll('strong') \n",
    "    for l in j :\n",
    "        jobs_oth.append(l.text)\n",
    "    return jobs_oth\n",
    "\n",
    "\n",
    "def firm_place(tabel) : # tjek den igen, der kommer lidt for mange elementer ud\n",
    "    firm_city=tabel.find_all('b')\n",
    "    #print(firm_city)\n",
    "    firm=[]\n",
    "    city=[]\n",
    "    cc=0\n",
    "    for i in firm_city:\n",
    "        if cc %2==0:\n",
    "            firm.append(i.text)\n",
    "        else :\n",
    "            city.append(i.text)\n",
    "        cc += 1\n",
    "    return firm, city\n",
    "\n",
    "\n",
    "def dates(tabel) : # finds only dates for others; needs also dates for own\n",
    "    dato_site=tabel.find_all('cite')\n",
    "\n",
    "    website=[]\n",
    "    for i in dato_site:\n",
    "        s_txt=i.text\n",
    "        site=s_txt.split(\",\")[0]\n",
    "        website.append(site)\n",
    "\n",
    "    indented=[]\n",
    "    for i in dato_site:\n",
    "        d_txt=i.text\n",
    "        dates=d_txt.split(\",\")\n",
    "        if dates[0] == \"StepStone\":    # Har behov for at blive fixet\n",
    "            indented.append(indented[-1])\n",
    "        else:\n",
    "            indented.append(dates[1])\n",
    "\n",
    "    monthval={'januar':'01','februar':'02', 'marts':'03', 'april':'04', 'maj':'05', 'juni':'06',\n",
    "              'juli':'07','august':'08','september':'09','oktober':'10','november':'11','december':'12'}\n",
    "    indented_d=[]\n",
    "\n",
    "    for i in indented:\n",
    "        datotal=(int(i.replace(i[3:-4], monthval.get(i[5:-5]))))\n",
    "        indented_d.append(datetime.strptime(str(datotal), '%d%m%Y'))\n",
    "    return indented_d\n",
    "\n",
    "\n",
    "\n",
    "#hyper = []\n",
    "#x1 = tabel.findAll('a', attrs={'href':True}) \n",
    "#for x in x1 :\n",
    "#    hyper.append(x.)\n",
    "#    \n",
    "#print(x1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49\n",
      "6\n",
      "6\n",
      "22\n",
      "21\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "def process_data() :\n",
    "    base_path = r\"C:\\Notebooks\\group19\\Eksamensprojekt/jobindex.txt\" # file with scraped jobindex data\n",
    "\n",
    "    f = open(base_path, 'r') # open the file for reading\n",
    "    \n",
    "    # loop through the file one line at a time\n",
    "    for html in f :\n",
    "        # here the various processing functions will be called\n",
    "        tabel, tabel1, tabel2= prepros(html)\n",
    "        \n",
    "        desc_own = job_descr_own(tabel2)\n",
    "        desc_oth = job_descr_oth(tabel1)\n",
    "        jobs_oth = job_title_oth(tabel)\n",
    "        firm, city = firm_place(tabel)\n",
    "        post_dates = dates(tabel)\n",
    "        \n",
    "        # der skal laves en zip-funktion, så de kan blive sat rigtig sammen\n",
    "    return desc_own, desc_oth, jobs_oth, firm, city, post_dates\n",
    "    f.close()\n",
    "\n",
    "desc_own, desc_oth, jobs_oth, firm, city, post_dates = process_data()\n",
    "print(len(desc_own))\n",
    "print(len(desc_oth))\n",
    "print(len(jobs_oth))\n",
    "print(len(firm))\n",
    "print(len(city))\n",
    "print(len(post_dates))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
