{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import requests, json\n",
    "import pandas as pd, numpy as np\n",
    "import time, os\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing the data\n",
    "\n",
    "The following steps parse the scraped html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The first cell defines our parsing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepros(html) :\n",
    "    d=json.loads(html)\n",
    "    \n",
    "    soup = BeautifulSoup(d['result_list_box_html'],'lxml')\n",
    "    \n",
    "    # this selects the main part of the html\n",
    "    tabel = soup.find('div', attrs={'class':'results component--default'})\n",
    "    \n",
    "    # Jobindex contain both own postings and postings from other\n",
    "    # the two are treated differently in the data\n",
    "    # Jobindex' imported postings\n",
    "    others = re.compile('r[\\d]+')\n",
    "    tabel1 = tabel.findAll('div', attrs={'data-beacon-tid':others})\n",
    "#    print(tabel1)\n",
    "\n",
    "    # Jobindex' own postings\n",
    "    own = re.compile('h[\\d]+')\n",
    "    tabel2 = tabel.findAll('div', attrs={'data-beacon-tid':own})\n",
    "    #print(tabel2)\n",
    "    \n",
    "    return tabel, tabel1, tabel2\n",
    "\n",
    "def job_descr_own(tabel2) : # den er færdig (næsten, der kommer lidt snask med, men det er ok)\n",
    "    desc_own = []\n",
    "    lineshift = re.compile('\\n')\n",
    "    # tag ul skal med \n",
    "    for l in tabel2 :\n",
    "        g1 = l.findAll('p') \n",
    "        g2 = l.findAll('li')\n",
    "        qs = ''\n",
    "#        print(g)\n",
    "        for m in g1 :\n",
    "            qs = qs + m.text + \". \" \n",
    "        for m in g2 :\n",
    "            if 'class=' not in m.text :\n",
    "                qs = qs + m.text + \". \" \n",
    "        desc_own.append(lineshift.sub(\"\", qs))\n",
    "    return desc_own\n",
    "\n",
    "def job_descr_oth(tabel1) : # færdig og virker\n",
    "    desc_oth = []\n",
    "    p=re.compile('\"')\n",
    "    lineshift = re.compile('\\n')\n",
    "    besk = []\n",
    "    for t_ in tabel1 :\n",
    "        besk.append(lineshift.sub(\"\", t_.text))\n",
    "    for t in besk :\n",
    "        s = t.split(sep='    ')\n",
    "        if len(s) == 1 :\n",
    "            desc_oth.append(\"\")\n",
    "        else :\n",
    "            desc_oth.append(t.split(sep='    ')[1])\n",
    "    return desc_oth\n",
    "    \n",
    "def job_title_oth (tabel) : # færdig og virker\n",
    "    # udled jobs andre\n",
    "    jobs_oth = []\n",
    "    j = tabel.findAll('strong') \n",
    "    for l in j :\n",
    "        jobs_oth.append(l.text)\n",
    "    return jobs_oth\n",
    "\n",
    "def job_title_own(tabel2) :\n",
    "    a = []\n",
    "    for l in tabel2 :\n",
    "        try :\n",
    "            a.append(l.findAll('b')[0].text)\n",
    "        except :\n",
    "            print(\"Der er ikke nogen stillingsbetegnelse\", '\\n')\n",
    "            print(l)\n",
    "            a.append('Ikke fundet')\n",
    "    return a\n",
    "    \n",
    "def firm_place(tabel) : # ok\n",
    "    firm=[]\n",
    "    city=[]\n",
    "    for tag in tabel :\n",
    "        firm_city=tag.findAll('b')\n",
    "        if len(firm_city) == 0 :\n",
    "            firm.append(\"Ukendt\")\n",
    "            city.append(\"Uoplyst\")\n",
    "        elif len(firm_city) == 1 :\n",
    "            firm.append(firm_city[0].text)\n",
    "            city.append(\" \")\n",
    "        else :\n",
    "            firm.append(firm_city[0].text)\n",
    "            city.append(firm_city[1].text)\n",
    "    return firm, city\n",
    "\n",
    "\n",
    "def dates(tabel) : # ok\n",
    "    indented_d=[]\n",
    "    for tag in tabel :\n",
    "        dato_site=tag.findAll('time')\n",
    "\n",
    "        monthval={'januar': 1,'februar': 2, 'marts': 3, 'april': 4, 'maj': 5, 'juni': 6, \\\n",
    "                  'juli': 7,'august': 8,'september': 9,'oktober': 10,'november': 11,'december': 12}\n",
    "\n",
    "        for i in dato_site:\n",
    "            t = i.text.split()\n",
    "#            try :\n",
    "#            datotal=(t[0][:-1]+ monthval.get(t[1])+ t[2])\n",
    "            indented_d.append(datetime(int(t[2]), int(monthval.get(t[1])), int(t[0][:-1])))\n",
    "#                indented_d.append(time.mktime(datetime.strptime(str(datotal), '%d%m%Y').timetuple()))\n",
    "#            except :\n",
    "#                indented_d.append(time.time()) # vi bør videreføre sidst kendte værdi også på tværs af sider\n",
    "    return indented_d\n",
    "\n",
    "def firms_own_fct(tabel2):\n",
    "    firms_own = []\n",
    "    for tag in tabel2 :\n",
    "        firms_egne=tag('img')\n",
    "        regex = re.compile('alt=\"(.*?)\" (?!border: 0px; margin: 0)')\n",
    "        firms_own_=regex.findall(str(firms_egne))\n",
    "        if len(firms_own_) == 0 :\n",
    "            firms_own_ = 'Ukendt'\n",
    "        firms_own.append(firms_own_[0])\n",
    "    return firms_own\n",
    "\n",
    "\n",
    "def cities_own(tabel2) :\n",
    "    city_own = []\n",
    "    for tag in tabel2 :\n",
    "        cit = tag('p')\n",
    "        regex = re.compile('</a>, (.+?)\\s*?</p>')\n",
    "        cities = regex.findall(str(cit))\n",
    "        if len(cities) == 0 :\n",
    "            cities = ['Uoplyst']\n",
    "        city_own.append(cities[0])\n",
    "    return city_own\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To test the parsing, we select a subsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start: 1535401974.650813 . Slut: 1535401984.9324229 . Linier hentet: 182\n",
      "Kørseltid: 10.28 sekunder\n"
     ]
    }
   ],
   "source": [
    "# select small subsample\n",
    "import random, time\n",
    "\n",
    "ca_samplesize = 200\n",
    "#base_path = r\"C:/Notebooks/jobindex.txt\"\n",
    "base_path = r\"C:\\Users\\pot\\Documents\\GitHub/jobindex.txt\"\n",
    "#sample_path = r\"C:/Notebooks/jobindex_sample.txt\"\n",
    "sample_path = r\"C:\\Users\\pot\\Documents\\GitHub/jobindex_sample.txt\"\n",
    "f = open(base_path,'r')\n",
    "s = open(sample_path, 'w')\n",
    "linienr = 0\n",
    "t0 = time.time()\n",
    "for line in f :\n",
    "    ran = random.uniform(0, 1) \n",
    "    if ran < ca_samplesize/17000 :\n",
    "        linienr += 1\n",
    "        s.write(line)\n",
    "f.close()\n",
    "t1 = time.time()\n",
    "print(\"Start:\", t0, \". Slut:\", t1, \". Linier hentet:\", linienr)\n",
    "print(\"Kørseltid:\", round(t1-t0,2), \"sekunder\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The function that governs the parsing and concatanate the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procestid er 14 minutter og 31.07 sekunder\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def process_data() :\n",
    "    t0 = time.time()\n",
    "    base_path = r\"C:\\notebooks\\jobindex4.txt\" # file with scraped jobindex data\n",
    "#    base_path = r\"C:\\Users\\pot\\Documents\\GitHub\\jobindex2.txt\" # file with scraped jobindex data\n",
    "    \n",
    "\n",
    "    f = open(base_path, mode='r', encoding='utf8') # open the file for reading # jobindex2 skal læses med , encoding='utf8'\n",
    "    count = 0\n",
    "    # loop through the file one line at a time\n",
    "\n",
    "    output_oth = pd.DataFrame(columns=['date', 'job_title', 'job_describ', 'city', 'company'])\n",
    "    output_own = pd.DataFrame(columns=['date', 'job_title', 'job_describ', 'city', 'company'])\n",
    "    \n",
    "   \n",
    "    for html in f :\n",
    "        count += 1\n",
    "#        print(count, len(html))\n",
    "        if len(html) == 1 :\n",
    "            continue\n",
    "        # here the various processing functions will be called\n",
    "        tabel, tabel1, tabel2= prepros(html)\n",
    "#        print(len(tabel))\n",
    "\n",
    "        desc_own = job_descr_own(tabel2)\n",
    "#        print(\"desc_own\", '\\n', len(desc_own), '\\n', desc_own)\n",
    "\n",
    "        desc_oth = job_descr_oth(tabel1)\n",
    "#        print('desc_oth', len(desc_oth), desc_oth)\n",
    "        \n",
    "        jobs_oth = job_title_oth(tabel)\n",
    "#        print('jobs_oth', len(jobs_oth), jobs_oth)\n",
    "        \n",
    "        firm_oth, city_oth = firm_place(tabel1)\n",
    "#        print(\"Andre firms:\", len(firm_oth))\n",
    "        \n",
    "        post_dates_oth = dates(tabel1)\n",
    "#        print(\"dates\", len(post_dates_oth))\n",
    "\n",
    "        post_dates_own = dates(tabel2)\n",
    "#        print(\"dates\", len(post_dates_own), post_dates_own)\n",
    "\n",
    "        firms_own = firms_own_fct(tabel2)\n",
    "#        print(\"Egne firms:\", len(firms_own), '\\n', firms_own)\n",
    "\n",
    "        city_own = cities_own(tabel2)\n",
    "#        print(\"Egne city:\", len(city_own), '\\n', city_own)\n",
    "#        print(\"\\n\")\n",
    "\n",
    "        jobs_own = job_title_own(tabel2)\n",
    "#        print(\"Egne jobtitler:\", len(jobs_own), '\\n', jobs_own)\n",
    "#        print(\"\\n\")\n",
    "\n",
    "    # der skal laves en zip-funktion, så de kan blive sat rigtig sammen\n",
    "#    return desc_own, desc_oth, jobs_oth, firm, city, post_dates\n",
    "        tmp = pd.DataFrame(list(zip(post_dates_oth, jobs_oth, desc_oth, city_oth, firm_oth)), \n",
    "                           columns=['date', 'job_title', 'job_describ', 'city', 'company'])\n",
    "        output_oth = pd.concat([output_oth, tmp], axis=0, join='inner', ignore_index=True)\n",
    " \n",
    "#        print(\"Dato:\", '\\n', post_dates_own, '\\n')\n",
    "#        print(\"Beskrivelse:\", '\\n', desc_own, '\\n')\n",
    "#        print(\"Byer:\", '\\n', city_own, '\\n')\n",
    "#        print(\"Firma:\", '\\n', firms_own, '\\n')\n",
    "    \n",
    "\n",
    "        tmp2 = pd.DataFrame(list(zip(post_dates_own, jobs_own, desc_own, city_own, firms_own)),\n",
    "                           columns=['date', 'job_title', 'job_describ', 'city', 'company'])\n",
    "        output_own = pd.concat([output_own, tmp2], axis=0, join='inner', ignore_index=True)\n",
    "    f.close()\n",
    "    t1 = time.time()\n",
    "    print(\"Procestid er\", int((t1-t0)/60), \"minutter og\", round((t1-t0)%60, 2), \"sekunder\")\n",
    "    #print(output_oth)\n",
    "    return output_oth, output_own\n",
    "\n",
    "output_oth, output_own = process_data()\n",
    "\n",
    "#Procestid er 17 minutter og 59.43 sekunder for both1\n",
    "#Procestid er 6 minutter og 31.98 sekunder for both2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saves the outcome with pandas pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(136062, 5)\n",
      "(36718, 5)\n",
      "(172780, 5)\n"
     ]
    }
   ],
   "source": [
    "# gem data\n",
    "\n",
    "print(output_oth.shape)\n",
    "print(output_own.shape)\n",
    "\n",
    "df_both = pd.concat([output_oth, output_own], axis=0, join='outer', ignore_index=True)\n",
    "print(df_both.shape)\n",
    "df_both.to_pickle('both4.pkl')\n",
    "\n",
    "#print(df_both.tail(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procedure to read the pandas pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape (471820, 5)\n",
      "\n",
      "\n",
      "        date                                          job_title  \\\n",
      "0 2009-12-31  ABB - Salgsingeniør / Lavspændingskomponenter ...   \n",
      "\n",
      "                                         job_describ                city  \\\n",
      "0  Til vores krævende og kompetente kunder som be...  Region Midtjylland   \n",
      "\n",
      "   company  \n",
      "0  ABB A/S   \n",
      "              date                   job_title  \\\n",
      "471818 2008-01-01  Løn- og Økonomimedarbejder   \n",
      "471819 2008-01-01        Regnskabsmedarbejder   \n",
      "\n",
      "                                              job_describ     city  \\\n",
      "471818  Strålfors Information Logistics A/S, Brøndby ....  Brøndby   \n",
      "471819  Entreprenørfirmaet Per Jørgensen ApS, Årslev ....  Uoplyst   \n",
      "\n",
      "                                     company  \n",
      "471818   Strålfors Information Logistics A/S  \n",
      "471819  Entreprenørfirmaet Per Jørgensen ApS  \n"
     ]
    }
   ],
   "source": [
    "# læs data \n",
    "ny1 = pd.read_pickle('both1.pkl')\n",
    "ny2 = pd.read_pickle('both2.pkl')\n",
    "\n",
    "df_both = pd.concat([ny1, ny2], axis=0, join='inner', ignore_index=True)\n",
    "\n",
    "print(\"Shape\", df_both.shape)\n",
    "print(\"\\n\")\n",
    "print(df_both.head(1), \"\\n\", df_both.tail(2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Region Midtjylland'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-6dcccca7d19c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mcities\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mobs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdf_both\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcity\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mcities\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mcompanies\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Region Midtjylland'"
     ]
    }
   ],
   "source": [
    "#for dates in pd.unique(df_both.date) :\n",
    "#    print(dates)\n",
    "\n",
    "cities = {}\n",
    "for obs in df_both.city :\n",
    "    cities[obs] += 1\n",
    "\n",
    "companies = {}\n",
    "for obs in df_both.company :\n",
    "    companies[obs] += 1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
