{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape Jobindex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import requests, json\n",
    "import pandas as pd, numpy as np\n",
    "import time, os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get web data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Procedure\n",
    "collect the first page\n",
    "save the totalresultcount for later usage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the basic url\n",
    "# https://www.jobindex.dk/jobsoegning.json?q=&jobage=archive&mindate=20180524&maxdate=20180824&page=2\n",
    "url = 'https://www.jobindex.dk/jobsoegning.json'\n",
    "\n",
    "\n",
    "# the request session\n",
    "session = requests.session()\n",
    "session.headers['email'] = 'pot@potweb.dk' \n",
    "session.headers['name'] = 'Peter Ottosen'\n",
    "#session.headers['User-Agent'] = '' # sometimes you need to pose as another agent...\n",
    "session.headers\n",
    "\n",
    "# container for jobpostings\n",
    "jobpostings = []\n",
    "done = []\n",
    "\n",
    "global last_t\n",
    "last_t = time.time()\n",
    "\n",
    "# function to create the next link\n",
    "def create_link(offset_) :\n",
    "    mindate = 20070101\n",
    "    maxdate = 20070630\n",
    "    addit = '?q=&jobage=archive&mindate=' + str(mindate) + '&maxdate=' + str(maxdate) + '&page=' + str(offset_)\n",
    "    return url + addit\n",
    "\n",
    "def ratelimit():\n",
    "    time.sleep(1) # sleep one second.\n",
    "\n",
    "# Reliable requests\n",
    "def get(url,iterations=10,check_function=lambda x: x.ok):\n",
    "    \"\"\"This module ensures that your script does not crash from connection errors.\n",
    "        that you limit the rate of your calls\n",
    "        that you have some reliability check\n",
    "        iterations : Define number of iterations before giving up. \n",
    "        exceptions: Define which exceptions you accept, default is all. \n",
    "    \"\"\"\n",
    "    for iteration in range(iterations):\n",
    "        try:\n",
    "            # add ratelimit function call here\n",
    "            ratelimit() # !!\n",
    "            response = session.get(url)\n",
    "            if response.ok:\n",
    "                return response # if succesful it will end the iterations here\n",
    "        except exceptions as e: #  find exceptions in the request library requests.exceptions\n",
    "            print(e) # print or log the exception message.\n",
    "    return None # your code will purposely crash if you don't create a check function later.\n",
    "\n",
    "def log_function(url,response,error_check=lambda x: x.ok,separator=',') :\n",
    "    global last_t\n",
    "#    logfilepath = r\"C:\\Users\\pot\\Documents\\GitHub\\group19\\Eksamensprojekt/log_jobindex3a.csv\"\n",
    "    logfilepath = r\"C:\\notebooks\\group19\\Eksamensprojekt/log_jobindex3a.csv\"\n",
    "    if os.path.isfile(logfilepath) :\n",
    "        f_log = open(logfilepath,'a')\n",
    "    else:\n",
    "        f_log = open(logfilepath,'w') # define logfile, remember not to overwrite it.\n",
    "        # write columns to be used, basic ones are, servertime, deltaT since last call, url, success of request, \n",
    "        header = ['serverTime','deltaT','url','success','length','path']\n",
    "        f_log.write(','.join(header)+'\\n')\n",
    "    #### Update timing info ####\n",
    "    t = time.time()\n",
    "    delta_t = t-last_t # calculate time since last call\n",
    "    last_t = t# update last call time\n",
    "    #### meta data ### \n",
    "    success = error_check(response)\n",
    "    if success: # if call is successfull we add it to the done container\n",
    "        done.append(url)\n",
    "    if response.ok:\n",
    "        length = len(response.text)\n",
    "    else:\n",
    "        length = 0\n",
    "    row = [t,delta_t,url,success,length,path]\n",
    "    f_log.write(separator.join(map(str,row))+'\\n')\n",
    "    f_log.close()\n",
    "\n",
    "\n",
    "# just to get the number of results\n",
    "#offset = 1\n",
    "count = 0\n",
    "#r = get(create_link(offset))\n",
    "#d=json.loads(r.text)\n",
    "\n",
    "#ResultCount=d['TotalResultCount']\n",
    "#jp_keys = d['JobPositionPostings'][0].keys()\n",
    "\n",
    "for pages in range(1, 19030) : # pagineringen\n",
    "    offset = pages\n",
    "    link = create_link(offset)\n",
    "    if link in done: #check if you have already downloaded the link\n",
    "        continue\n",
    "    count +=1\n",
    "    r = get(link)\n",
    "    d=json.loads(r.text)\n",
    "#    jobpostings.append(d['JobPositionPostings'])\n",
    "\n",
    "    #define path \n",
    "#    base_path = r\"C:\\Users\\pot\\Documents\\GitHub/jobindex3.txt\"\n",
    "    base_path = r\"C:\\notebooks/jobindex3a.txt\"\n",
    "    path = base_path      # use the link count as a filename\n",
    "    if r.ok:\n",
    "        if os.path.isfile(path) :\n",
    "            f = open(path, mode=\"a\", encoding=\"utf8\")\n",
    "        else:\n",
    "            f = open(path, mode=\"w\", encoding=\"utf8\") # define logfile, remember not to overwrite it.\n",
    "            \n",
    "        html = r.text\n",
    "        try :\n",
    "            f.write(html)\n",
    "            f.write('\\n')\n",
    "        except UnicodeEncodeError :\n",
    "            print(\"Der opstod en UnicodeEncodeError ved skrivning af side\", str(pages))\n",
    "            f.write(html)\n",
    "            f.write('\\n')\n",
    "        else :\n",
    "            pass\n",
    "        f.close()\n",
    "    # run your log function\n",
    "    log_function(link, r)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
